apiVersion: v1
kind: ConfigMap
metadata:
  name: opentelemetry-rules
  namespace: alert-migrator-test
  labels:
    app: prometheus
  annotations:
    prometheus.io/kube-rules: "true"
data:
  receiver_spans_refused_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_receiver_refused_spans{app="splunk-otel-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      description: "High number of refused spans in OpenTelemetry collector receiver (Splunk)"
      causes: "Service is most likely unhealthy, investigate the metric and identify which is the affected receiver"
  receiver_spans_refused_collector: |
    alert: Refused_Metrics_Points_Opentelemetry_Collector
    expr: rate(otelcol_receiver_refused_spans{app="opentelemetry-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
      test: "test"
    annotations:
      description: "High number of refused spans points in OpenTelemetry collector receiver (Splunk)"
      causes: "Service is most likely unhealthy, investigate the metric and identify which is the affected receiver"
  exporter_metrics_failed_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_exporter_enqueue_failed_metric_points{app="splunk-otel-collector"}[5m]) > 5
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      description: "The Splunk OpenTelemetry collector is failing to export metrics with the following protocol {% raw %}{{ $labels.exporter }}{% endraw %}"
      causes: "Service is most likely unhealthy"
  exporter_metrics_failed_collector: |
    alert: Refused_Metrics_Points_Opentelemetry_Collector
    expr: rate(otelcol_exporter_enqueue_failed_metric_points{app="opentelemetry-collector"}[5m]) > 1
    for: 4m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      description: "The OpenTelemetry collector is failing to export metrics with the following protocol {% raw %}{{ $labels.exporter }}{% endraw %}"
      causes: "Service is most likely unhealthy test"
  exporter_spans_failed_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_exporter_enqueue_failed_spans{app="splunk-otel-collector"}[5m]) > 5
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      description: "The Splunk OpenTelemetry collector is failing to export spans with the following protocol {% raw %}{{ $labels.exporter }}{% endraw %}"
      causes: "Service is most likely unhealthy"
  all_instances_down_otel_collector: |
    alert: Opentelemetry_Collector_Downq
    expr: sum(up{app="opentelemetry-collectordsd", job="kubernetes-pods"}) == 1 or absent(up{app="opentelemetry-collector", job="kubernetes-pods"}) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      description: "The OpenTelemetry collector has been down for more than 3 minutes yotam"
      causes: "Service is most likely down or fails healthchecks"
  all_instances_down_splunk_collectors: |
    alert: Splunk_Collector_Down
    expr: sum(up{app="splunk-otel-collectorsd", job="kubernetes-pods"}) == 1 or absent(up{app="splunk-otel-collector", job="kubernetes-pods"}) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "majors"
    annotations:
      description: "The Splunk OpenTelemetry collector has been down for more than 3 minutes"
      causes: "Service is most likely down or fails healthchecks"
  receiver_metrics_points_refused_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_receiver_refused_metric_points{app="splunk-otel-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      description: "High number of refused metric points in OpenTelemetry collector receiver fsdfdsff"
      causes: "Service is most likely unhealthy, investigate the metric and identify which is the affected transport protocol (http, grpc, otlp, etc)"






apiVersion: v1
kind: ConfigMap
metadata:
  name: opentelemetry-rules
  namespace: monitoring
  labels:
    app: prometheus
  annotations:
    prometheus.io/kube-rules: "true"
data:
  all_instances_down_otel_collector: |
    alert: Opentelemetry_Collector_Down
    expr: sum(up{app="opentelemetry-collector", job="kubernetes-pods"}) == 0 or absent(up{app="opentelemetry-collector", job="kubernetes-pods"}) > 0
    for: 3m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "The OpenTelemetry collector has been down for more than 3 minutes"
      causes: "Service is most likely down or fails healthchecks"
  all_instances_down_splunk_collector: |
    alert: Splunk_Collector_Down
    expr: sum(up{app="splunk-otel-collector", job="kubernetes-pods"}) == 0 or absent(up{app="splunk-otel-collector", job="kubernetes-pods"}) > 0
    for: 3m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "The Splunk OpenTelemetry collector has been down for more than 3 minutes"
      causes: "Service is most likely down or fails healthchecks"
  receiver_metrics_points_refused_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_receiver_refused_metric_points{app="splunk-otel-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "High number of refused metric points in OpenTelemetry collector receiver"
      causes: "Service is most likely unhealthy, investigate the metric and identify which is the affected transport protocol (http, grpc, otlp, etc)"
  receiver_metrics_points_refused_collector: |
    alert: Refused_Metrics_Points_Opentelemetry_Collector
    expr: rate(otelcol_receiver_refused_metric_points{app="opentelemetry-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "High number of refused metric points in OpenTelemetry collector receiver"
      causes: "Service is most likely unhealthy, investigate the metric and identify which is the affected receiver"
  receiver_spans_refused_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_receiver_refused_spans{app="splunk-otel-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "High number of refused spans in OpenTelemetry collector receiver (Splunk)"
      causes: "Service is most likely unhealthy, investigate the metric and identify which is the affected receiver"
  receiver_spans_refused_collector: |
    alert: Refused_Metrics_Points_Opentelemetry_Collector
    expr: rate(otelcol_receiver_refused_spans{app="opentelemetry-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "High number of refused spans points in OpenTelemetry collector receiver (Splunk)"
      causes: "Service is most likely unhealthy, investigate the metric and identify which is the affected receiver"
  exporter_metrics_failed_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_exporter_enqueue_failed_metric_points{app="splunk-otel-collector"}[5m]) > 5
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "The Splunk OpenTelemetry collector is failing to export metrics with the following protocol {% raw %}{{ $labels.exporter }}{% endraw %}"
      causes: "Service is most likely unhealthy"
  exporter_metrics_failed_collector: |
    alert: Refused_Metrics_Points_Opentelemetry_Collector
    expr: rate(otelcol_exporter_enqueue_failed_metric_points{app="opentelemetry-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "The OpenTelemetry collector is failing to export metrics with the following protocol {% raw %}{{ $labels.exporter }}{% endraw %}"
      causes: "Service is most likely unhealthy"
  exporter_spans_failed_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_exporter_enqueue_failed_spans{app="splunk-otel-collector"}[5m]) > 5
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "The Splunk OpenTelemetry collector is failing to export spans with the following protocol {% raw %}{{ $labels.exporter }}{% endraw %}"
      causes: "Service is most likely unhealthy"
  exporter_spans_failed_collector: |
    alert: Refused_Metrics_Points_Opentelemetry_Collector
    expr: rate(otelcol_exporter_enqueue_failed_spans{app="opentelemetry-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      dashboard: "https://grafana.tooling-production.poppulo-tooling.com/d/BKf2sowmj/opentelemetry-collector?orgId=1&refresh=10s&var-datasource={{ GRAFANA_PROMETHEUS }}&var-minstep=%24__auto_interval_minstep&var-receiver=All&var-processor=All&var-exporter=All&var-app=splunk-otel-collector&from=1676453659364&to=1676475259364"
      description: "The OpenTelemetry collector is failing to export spans with the following protocol {% raw %}{{ $labels.exporter }}{% endraw %}"
      causes: "Service is most likely unhealthy"





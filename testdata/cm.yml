apiVersion: v1
kind: ConfigMap
metadata:
  name: opentelemetry-rules
  namespace: alert-migrator-test
  labels:
    app: prometheus
  annotations:
    prometheus.io/kube-rules: "true"
data:
  all_instances_down_otel_collector_yotams: |
    alert: Opentelemetry_Collector_Downq
    expr: sum(up{app="opentelemetry-collectord", job="kubernetes-pods"}) == 0 or absent(up{app="opentelemetry-collector", job="kubernetes-pods"}) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      description: "The OpenTelemetry collector has been down for more than 3 minutes yotam"
      causes: "Service is most likely down or fails healthchecks"
  all_instances_down_splunk_collectors: |
    alert: Splunk_Collector_Down
    expr: sum(up{app="splunk-otel-collector", job="kubernetes-pods"}) == 0 or absent(up{app="splunk-otel-collector", job="kubernetes-pods"}) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "majors"
    annotations:
      description: "The Splunk OpenTelemetry collector has been down for more than 3 minutes"
      causes: "Service is most likely down or fails healthchecks"
  receiver_metrics_points_refused_splunk: |
    alert: Refused_Metrics_Points_Splunk
    expr: rate(otelcol_receiver_refused_metric_points{app="splunk-otel-collector"}[5m]) > 0
    for: 5m
    labels:
      team: "sre"
      severity: "major"
    annotations:
      description: "High number of refused metric points in OpenTelemetry collector receiver fsdfdsff"
      causes: "Service is most likely unhealthy, investigate the metric and identify which is the affected transport protocol (http, grpc, otlp, etc)"





